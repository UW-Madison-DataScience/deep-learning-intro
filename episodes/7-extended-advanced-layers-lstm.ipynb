{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d25247",
   "metadata": {},
   "source": [
    "# Advanced layer types: Sequences and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94fefc",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- Why do we need layers specifically designed for sequential data?\n",
    "- What are Recurrent Neural Networks (RNNs) and LSTMs?\n",
    "- How does an LSTM \"remember” important information over time?\n",
    "- What are alternatives like attention?:::\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand the structure and motivation behind RNN and LSTM layers\n",
    "- Relate LSTM concepts to earlier architectures (dense, CNN)\n",
    "- Explore a simple forecasting example using LSTM:::\n",
    "\n",
    "## Sequences are not just lists\n",
    "\n",
    "In the previous episodes, we dealt with **tabular data** (e.g. penguins), and **image data** (e.g. Dollar Street), where the model processed inputs as independent examples.\n",
    "\n",
    "But what happens when the **order of the data matters**?\n",
    "\n",
    "- In **weather forecasting**, yesterday's temperature helps predict today's.\n",
    "- In **text**, the word before \"bank\" helps us decide whether it's about money or rivers.\n",
    "- In **biology**, DNA and protein sequences have important patterns based on order.\n",
    "\n",
    "We need a model that can **use previous inputs to inform current predictions**. That's where **Recurrent Neural Networks (RNNs)** and **Long Short-Term Memory (LSTM)** layers come in.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## From Dense to Recurrent\n",
    "\n",
    "In a dense layer, the network sees all the input at once. Each neuron connects to every input value.\n",
    "\n",
    "In a CNN, filters move across the input (image or sequence), learning local patterns and reducing connections.\n",
    "\n",
    "In a vanilla RNN, the network processes one input step at a time and reuses the same layer at each time step. What makes it \"recurrent\" is that it passes a hidden state between steps.\n",
    "\n",
    "\n",
    "\n",
    "### Recurrent loop: how a vanilla RNN works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2922b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "┌────────────┐\n",
    "x_t ───►  │  RNN cell  │ ───► h_t\n",
    "          └────────────┘\n",
    "            ▲\n",
    "         h_{t-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fad88",
   "metadata": {},
   "source": [
    "At each time step t, the RNN takes:\n",
    "- the input at this step x_t\n",
    "- the hidden state from the previous step h_{t-1}\n",
    "\n",
    "It produces a new hidden state h_t, which is used for the next time step and sometimes for prediction.\n",
    "\n",
    "This creates a short-term memory loop across the sequence.\n",
    "\n",
    "\n",
    "## The problem with vanilla RNNs\n",
    "\n",
    "Basic RNNs can capture short-term dependencies, but they struggle to retain information across long sequences — a limitation known as the vanishing gradient problem.\n",
    "\n",
    "Imagine trying to predict the next word in a sentence:\n",
    "\n",
    "> I grew up in France… I speak fluent ___.\n",
    "\n",
    "You want the model to remember \"France\" — even if it happened many steps earlier. Vanilla RNNs often forget these long-range dependencies.\n",
    "\n",
    "\n",
    "\n",
    "## LSTM to the rescue\n",
    "\n",
    "LSTM (Long Short-Term Memory) layers address this by adding a memory component: the cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "┌────────────┐\n",
    "x_t ───►  │  LSTM cell │ ───►   h_t\n",
    "          └────────────┘\n",
    "            ▲       ▲\n",
    "        h_{t-1}   c_{t-1} (memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701ac1d",
   "metadata": {},
   "source": [
    "At each time step t, the LSTM takes:\n",
    "- the input x_t\n",
    "- the previous hidden state h_{t-1}\n",
    "- the previous cell state c_{t-1}\n",
    "\n",
    "The cell state acts as long-term memory, while the hidden state provides a short-term summary. Gates inside the LSTM control how much information to forget, store, or expose.\n",
    "\n",
    "- **Forget gate**: What information should be erased from memory?\n",
    "- **Input gate**: What new information should be stored?\n",
    "- **Output gate**: What part of the memory should be passed forward?\n",
    "\n",
    "This lets the model maintain a persistent internal state across many steps.\n",
    "\n",
    "Unlike CNNs, LSTMs are not spatially structured, but **temporally structured**: they are great for time, text, or biological sequences.\n",
    "\n",
    "\n",
    "## LSTM example: Forecasting temperature\n",
    "\n",
    "Let's use the daily minimum temperature dataset from Melbourne (1981–1990)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
    "df = pd.read_csv(url, parse_dates=[\"Date\"])\n",
    "\n",
    "# Normalize and prepare sequences\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(df[\"Temp\"].values.reshape(-1, 1))\n",
    "\n",
    "def make_sequences(data, window=7):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window):\n",
    "        X.append(data[i:i+window])\n",
    "        y.append(data[i+window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = make_sequences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4b09d",
   "metadata": {},
   "source": [
    "Train a basic LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d65108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(X.shape[1], X.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, y, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f8dd0",
   "metadata": {},
   "source": [
    "## Tying it back to earlier episodes\n",
    "\n",
    "| Episode | What we did              | How LSTM builds on it                     |\n",
    "|--------|--------------------------|-------------------------------------------|\n",
    "| Dense  | Connected all inputs     | Still used inside LSTM layers             |\n",
    "| CNN    | Shared filters for space | LSTM shares memory over time              |\n",
    "| Dropout| Prevent overfitting      | Also used inside LSTM to regularize       |\n",
    "| Optimizer | Used Adam              | Same optimizer, but gradients are harder  |\n",
    "\n",
    "\n",
    "## Attention: A preview\n",
    "\n",
    "LSTMs read inputs in order. But **attention** allows the model to \"look around” the input and learn which parts are important — like reading with a highlighter.\n",
    "\n",
    "In Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c72d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, MultiHeadAttention, LayerNormalization\n",
    "\n",
    "inputs = Input(shape=(100, 64))\n",
    "attn_output = MultiHeadAttention(num_heads=2, key_dim=32)(inputs, inputs)\n",
    "attn_output = LayerNormalization()(inputs + attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba5536",
   "metadata": {},
   "source": [
    "You don't have to understand this fully yet — just know that **attention** is a different strategy for processing sequences, used in Transformers and large language models (LLMs).\n",
    "\n",
    "\n",
    "\n",
    "::: challenge\n",
    "What are examples of sequence data in your domain?\n",
    "- What would be a reasonable input representation?\n",
    "- Would the order matter?\n",
    "- Would a CNN, dense model, or LSTM make the most sense?\n",
    ":::\n",
    "\n",
    "## Keypoints\n",
    "\n",
    "- RNNs and LSTMs allow neural networks to process data step-by-step\n",
    "- LSTMs retain long-term context using gated memory\n",
    "- Sequence models are widely used in time series, language, and biology:::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
