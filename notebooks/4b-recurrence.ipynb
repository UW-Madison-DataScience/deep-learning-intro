{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d25247",
   "metadata": {},
   "source": [
    "# Advanced layer types: Recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94fefc",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- Why do we need layers specifically designed for sequential data?\n",
    "- What are Recurrent Neural Networks (RNNs) and LSTMs?\n",
    "- How does an LSTM \"remember” important information over time?\n",
    "- What are alternatives like attention?\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand the structure and motivation behind RNN and LSTM layers\n",
    "- Relate LSTM concepts to earlier architectures (dense, CNN)\n",
    "- Explore a simple forecasting example using LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b362b5d-6dae-4e21-8062-8e33253f0c81",
   "metadata": {},
   "source": [
    "## Revisiting sunshine hours\n",
    "\n",
    "Yesterday, we predicted today's sunshine hours (in Basel) using weather variables from just yesterday — a one-to-one mapping. Each input was a single day's data. Let's rebuild that model quickly to remind ourselves of the test set performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94eca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca05218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename_data = \"data/weather_prediction_dataset_light.csv\"\n",
    "data = pd.read_csv(filename_data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb45359-53ff-4588-bb4e-710d8720d1cd",
   "metadata": {},
   "source": [
    "### Select Basel features only\n",
    "To speed up our computation and simplify this model, we'll just focus on predictors from Basel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cd7fa-5adf-44cf-97ef-17fdcf6386c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only Basel-specific predictors\n",
    "basel_columns = [col for col in data.columns if col.startswith('BASEL_')]\n",
    "\n",
    "nr_rows = 9*365 # using 9 years to match our example on day 2\n",
    "\n",
    "# Drop DATE and MONTH, keep Basel predictors\n",
    "X_data = data.loc[:nr_rows, basel_columns]\n",
    "y_data = data.loc[1:(nr_rows + 1), \"BASEL_sunshine\"]\n",
    "print(X_data.shape)\n",
    "X_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3927d-a691-4304-a3d4-166b4941ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data.head() # show's next day of sunshine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099059f5-cd22-4738-b92a-c3020bde26bc",
   "metadata": {},
   "source": [
    "### Temporal train/test split\n",
    "This time, we'll apply a more appropriate split for a temporal dataset. We'll turn off shuffling so that the test set consists only of later time points than those in the training set. This setup allows us to evaluate how well the model can predict future values using only past information — without accidentally training on data from the future. This is important, because, the future often doesn't resemble the past, and we want to see how well our model can handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab94c7-f53f-4ab4-a1f1-4159929fe5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_set_size = 0.2\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X_data, y_data, test_size=test_set_size, random_state=0, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=0, shuffle=False)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634617b-86a1-4f33-be94-cb4610d12779",
   "metadata": {},
   "source": [
    "## Compute baseline performance: tomorrow's sunshine is equal to today's\n",
    "Since we're using a new train/test split, it's a good idea to re-compute the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f2358-dca2-4aa5-921d-640730dc8b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_baseline_prediction = X_test['BASEL_sunshine']\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse_baseline = root_mean_squared_error(y_test, y_baseline_prediction)\n",
    "print('Baseline:', rmse_baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf71a67-767d-42c7-af68-efffea47148b",
   "metadata": {},
   "source": [
    "## Recompute dense model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d24190-cfd6-4fce-b2b4-ed4a5d72d697",
   "metadata": {},
   "source": [
    "### Set random seed\n",
    "Set seeds to control for random weight initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff174c96-f7d8-4daf-a2a1-86041c68d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0a964-36bc-41c8-8eea-700d0b786547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def create_dense_nn(input_shape):\n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=input_shape, name='input')\n",
    "\n",
    "    # Dense layers\n",
    "    layers_dense = keras.layers.Dense(100, 'relu')(inputs)\n",
    "    layers_dense = keras.layers.Dense(50, 'relu')(layers_dense)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = keras.layers.Dense(1)(layers_dense)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=\"dense_weather_prediction_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14edfb-6694-4c7f-b26f-4ecee50ae171",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense = create_dense_nn(input_shape=(X_train.shape[1],))\n",
    "model_dense.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158dc090-9c45-467f-891d-4161191fae67",
   "metadata": {},
   "source": [
    "With just 9 input features, this fully connected model has 6,101 parameters.\n",
    "\n",
    "**Challenge**: What is the data:params ratio for this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96531e3-bb5b-4411-9b03-516bd1025b4a",
   "metadata": {},
   "source": [
    "**Challenge**: Is this an \"underparameterized\" or \"overparameterized\" model? How do you think it will perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c3415-9f3f-4490-893f-8ec1cc5ce48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse',\n",
    "                  metrics=[keras.metrics.RootMeanSquaredError()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8d77c-e184-4040-9be5-e475eb56798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model(model_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725dc763-771b-415c-aa14-720966e8b9e2",
   "metadata": {},
   "source": [
    "Fit with early stopping, as we did previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6930f70-f1e3-420f-87f7-7a998e6b4f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystopper = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10\n",
    "    )\n",
    "\n",
    "history_dense = model_dense.fit(X_train, y_train,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 200,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8efbee-5d0d-4ed0-a22e-4cd6c8061ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history, metrics):\n",
    "    \"\"\"\n",
    "    Plot the training history\n",
    "\n",
    "    Args:\n",
    "        history (keras History object that is returned by model.fit())\n",
    "        metrics (str, list): Metric or a list of metrics to plot\n",
    "    \"\"\"\n",
    "    history_df = pd.DataFrame.from_dict(history.history)\n",
    "    sns.lineplot(data=history_df[metrics])\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"metric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aab70f-4a2b-4caf-afdd-3c95f3227db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_dense, ['root_mean_squared_error', 'val_root_mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112f2d4-2e06-48a9-9a9d-b1e85df55236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcee0a-5717-456f-8709-c65eeba7d6eb",
   "metadata": {},
   "source": [
    "If you recall, our baseline model had an RMSE of Baseline: 4.27. Our fully connect neural network does slightly better than baseline (3.59). While it's encouraging to see an improvement to the baseline, there is much more we can do to improve this result. \n",
    "\n",
    "Recall that here, we are only looking at the current day's weather (across cities) to determine sunshine hours in Basel the next day. But what if sunshine patterns depend on the past week, past month, or past year? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc3ea1-6120-4ae6-919f-8a88912145fe",
   "metadata": {},
   "source": [
    "### When is a single lag not enough?\n",
    "\n",
    "In many real-world tasks, yesterday's data alone isn't sufficient — patterns unfold over time:\n",
    "\n",
    "- Rainy streaks often last several days\n",
    "- Cold fronts move gradually, not all at once\n",
    "- In other domains: heartbeats, language, gestures, and biological sequences like proteins rely on order and context across multiple steps\n",
    "\n",
    "#### Include mutliple lags manually?\n",
    "A natural next step is to **include multiple lags manually** as input features. \n",
    "\n",
    "For example: add sunshine hours from the last 30 days as separate columns. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942dd14-3c3f-42e4-8406-86c566527fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create lagged features for all Basel-specific predictors\n",
    "data_lagged = data.copy()\n",
    "basel_columns = [col for col in data.columns if col.startswith('BASEL_')]\n",
    "\n",
    "# Add lags for each Basel predictor (store in dictionary first)\n",
    "window_size = 30\n",
    "lagged_features = {}\n",
    "\n",
    "for col in basel_columns:\n",
    "    for lag in range(1, window_size + 1):\n",
    "        lagged_features[f'{col}_lag{lag}'] = data[col].shift(lag)\n",
    "\n",
    "# Concatenate all lagged features at once\n",
    "data_lagged = pd.concat([data_lagged, pd.DataFrame(lagged_features)], axis=1)\n",
    "\n",
    "# Drop rows with NaNs caused by lagging\n",
    "data_lagged = data_lagged.dropna().reset_index(drop=True)\n",
    "\n",
    "# Define X and y using only lagged Basel features\n",
    "all_lagged_cols = [col for col in data_lagged.columns if col.startswith('BASEL_') and 'lag' in col]\n",
    "X_data_lagged = data_lagged.loc[:nr_rows, all_lagged_cols]\n",
    "y_data_lagged = data_lagged.loc[:nr_rows, 'BASEL_sunshine']  # target is same as before\n",
    "\n",
    "# Train/validation/test split (preserving time order)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_lagged, X_holdout_lagged, y_train_lagged, y_holdout_lagged = train_test_split(\n",
    "    X_data_lagged, y_data_lagged, test_size=test_set_size, random_state=0, shuffle=False)\n",
    "\n",
    "X_val_lagged, X_test_lagged, y_val_lagged, y_test_lagged = train_test_split(\n",
    "    X_holdout_lagged, y_holdout_lagged, test_size=0.5, random_state=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2c8c5-86e3-4ff0-918f-8463b71bdf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lagged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb844f39-2961-4013-878e-229d1d9c89a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_lagged.shape)\n",
    "print(X_test_lagged.shape)\n",
    "print(X_val_lagged.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704f59a-be21-47f7-85a8-0ba2bcf46140",
   "metadata": {},
   "source": [
    "**Note**: Number of predictors grows drastically! How do you think this will impact model performance? \n",
    "\n",
    "Let's try it and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68fb19-1d66-4b1f-abe8-93ce3f3d15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dense model with additional input features\n",
    "model_dense_lagged = create_dense_nn(input_shape=(X_train_lagged.shape[1],))\n",
    "\n",
    "# view model summary\n",
    "model_dense_lagged.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb704cb9-725b-4764-9591-e011cc481972",
   "metadata": {},
   "source": [
    "**Note**: With 270 input features, the total number of weights increases dramaticaly as well. 32,201 (30 lags) >> 6,101 (1 lag)\n",
    "\n",
    "Compile and train the model next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47450062-5545-40de-b005-3b1238506928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compile model \n",
    "compile_model(model_dense_lagged)\n",
    "# train model and store results\n",
    "history_dense_lagged = model_dense_lagged.fit(X_train_lagged, y_train_lagged,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 200,\n",
    "                    validation_data=(X_val_lagged, y_val_lagged),\n",
    "                    callbacks=[earlystopper])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d67fe-e321-403e-a978-cb3a95236498",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_dense_lagged, ['root_mean_squared_error', 'val_root_mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6ea63-9347-4b6b-b6dd-f0db487d8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense_lagged.evaluate(X_test_lagged, y_test_lagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d39741-9fd2-454e-8766-e00269f587e4",
   "metadata": {},
   "source": [
    "\n",
    "### Discuss: What might be happening here?\n",
    "We see that the explosion in input parameters has made the problem more challenging to model. We get a test set error of 3.96 compared to 3.59 in our previous 1-lag dense model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f470b45-07cd-482f-9801-d8304adbd81c",
   "metadata": {},
   "source": [
    "### Why manual lagging isn't enough\n",
    "\n",
    "Previously, we gave our models access to past information by manually adding lagged versions of each predictor. While this works in principle, it introduces several problems:\n",
    "\n",
    "- **No awareness of time**: The model treats each lag (e.g., x_{t-1}, x_{t-30}) as a separate feature, with no sense that lag 1 is more recent than lag 30.\n",
    "- **Feature explosion**: Including many lags across many predictors causes the input size to grow rapidly — in our case, 30 lags × 9 features = 270 inputs.\n",
    "- **Manual choices**: You have to decide how many lags to include and which features to use. These decisions are often arbitrary and may not generalize.\n",
    "- **Separate weights for each lag**: A dense model learns a different set of weights for each timestep, which limits its ability to recognize repeating or shifting patterns.\n",
    "\n",
    "This approach flattens time into a wide input vector, forcing the model to memorize temporal structure instead of modeling it directly.\n",
    "\n",
    "## Introducing recurrence\n",
    "\n",
    "Recurrent neural networks (RNNs) address these limitations by processing one timestep at a time while maintaining a **hidden state** that evolves as the sequence progresses.\n",
    "\n",
    "At each timestep `t`, the RNN:\n",
    "- Receives the input `x_t`\n",
    "- Uses the hidden state from the previous timestep, `h_{t-1}`\n",
    "- Computes a new hidden state `h_t` using a shared transformation\n",
    "\n",
    "This process is called **recurrence** — the same computation (with shared parameters) is applied at every timestep. The hidden state acts as a running summary of what the model has seen so far.\n",
    "\n",
    "This is illustrated below:  \n",
    "![Unrolled RNN](rnn_unfolded.png)\n",
    "\n",
    "- Left: A single RNN cell, showing how it uses `x_t` and `h_{t-1}` to produce `o`\n",
    "- Right: The same cell **unrolled through time**, with the hidden state passed from one step to the next\n",
    "\n",
    "By the end of the sequence, the final hidden state reflects the model's accumulated understanding of everything it's seen.\n",
    "\n",
    "### Why RNNs don't need lagged features\n",
    "\n",
    "Rather than handing the model 30 manually lagged inputs, we give it the full sequence and let it learn what to remember over time. The hidden state captures temporal context dynamically — often retaining short-term memory well, but struggling with long-range dependencies due to how gradients behave during training (more on that later).\n",
    "\n",
    "RNNs resemble a *first-order Markov process*: each hidden state `h_t` depends only on `h_{t-1}` and `x_t`. But unlike fixed-state Markov models, RNNs learn a continuous, task-specific representation of state.\n",
    "\n",
    "Key benefits:\n",
    "- **Fewer parameters**: the same weights are reused across time\n",
    "- **Built-in temporal structure**: the model processes sequences step by step\n",
    "- **Better generalization**: no need to manually engineer lag features\n",
    "\n",
    "### How can one set of weights model an entire sequence?\n",
    "\n",
    "A common point of confusion: if the RNN uses the same weights at each timestep, how can it handle complex, time-dependent patterns?\n",
    "\n",
    "The answer is that the model doesn't try to learn the entire sequence at once. Instead, it learns **how to update a memory (the hidden state)** at each step based on the current input. The recurrence isn't solving the whole task at every point in time — it's gradually evolving a compact summary of the sequence.\n",
    "\n",
    "Each `h_t` is shaped by both `x_t` and everything that came before it, as encoded in `h_{t-1}`. The final hidden state reflects this cumulative process.\n",
    "\n",
    "## Building a recurrent model\n",
    "\n",
    "Recurrent layers expect 3D input: one sequence per sample. Instead of flattening the last 30 days into a single row, we reshape our data into sequences of timesteps, where each timestep contains a feature vector.\n",
    "\n",
    "At each step, the RNN:\n",
    "- Receives the current input `x_t`\n",
    "- Uses the previous hidden state `h_{t-1}`\n",
    "- Computes a new hidden state `h_t`\n",
    "\n",
    "This `h_t` can be passed forward to the next timestep or used to make a prediction — typically just once at the final step in many-to-one setups.\n",
    "\n",
    "### Preparing the inputs and outputs\n",
    "\n",
    "To train a recurrent model, we’ll transform our dataset so that each training sample is a short sequence of consecutive days (e.g., 30 timesteps) with the same set of predictors.\n",
    "\n",
    "Recurrent layers expect input tensors shaped:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c884d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = data.loc[:nr_rows].drop(columns=[\"DATE\", \"MONTH\"]) # nr_rows gives us 9 years of data again\n",
    "y_data = data.loc[window_size:(nr_rows + window_size)][\"BASEL_sunshine\"] # predict starting with first day after window_size\n",
    "X_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f021cd-c0ed-4552-8610-52a2f54672eb",
   "metadata": {},
   "source": [
    "Keep only BASEL predictors, as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d949776-9f5d-4d21-b031-8c389f026293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only BASEL predictors \n",
    "basel_columns = [col for col in data.columns if col.startswith('BASEL_')]\n",
    "\n",
    "# Drop NaNs and reset index to keep it tidy\n",
    "data_basel = data[basel_columns].dropna().reset_index(drop=True)\n",
    "data_basel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df8de0-460b-4f23-a35d-7bd347ba884a",
   "metadata": {},
   "source": [
    "#### How are the input sequences and targets constructed?\n",
    "\n",
    "Each input sample is a sequence of 30 consecutive days of predictor values.  \n",
    "We slide this 30-day window over the dataset to create many such sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb014a8-0ada-400f-a1f6-fc84465493b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Build sequences\n",
    "n_seq = len(data_basel) - window_size\n",
    "\n",
    "X_seq = np.stack([\n",
    "    data_basel[basel_columns].iloc[i:i+window_size].values\n",
    "    for i in range(n_seq)\n",
    "])\n",
    "print(\"X_seq shape:\", X_seq.shape)  # (samples, 30, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2158f-fe9b-43b1-9a0f-e3fbcc54116e",
   "metadata": {},
   "source": [
    "This creates a 3D array of shape (n_seq, window_size, features). Each slice `X_seq[i]` contains predictors for days i through i+window_size\n",
    "\n",
    "\n",
    "For each input sequence, we want the model to predict the sunshine on the day after the last timestep (i.e., day i+window_size). We align the target values accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bbe71-0963-43ce-9947-35a8f449d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_seq = data_basel['BASEL_sunshine'].iloc[window_size:].values # index starts at 0, so this gives us the 31st day as first data point in y_seq\n",
    "\n",
    "print(\"y_seq shape:\", y_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac4f1d-e13f-4525-9146-f481df950292",
   "metadata": {},
   "source": [
    "Temporal train/test split (without shuffling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X_seq, y_seq, test_size=test_set_size, random_state=0, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=0, shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a108a6bd-3b2c-4072-aff3-77d4d6885332",
   "metadata": {},
   "source": [
    "### Modeling sequences with a recurrent neural network\n",
    "\n",
    "Now that our input data is structured as sequences (30 timesteps × 9 features), we can use a recurrent neural network to learn from patterns over time.\n",
    "\n",
    "Instead of manually adding lagged features, we feed the model the full sequence and let it learn which parts of the past are useful. This keeps the input compact while enabling the model to capture temporal dependencies internally.\n",
    "\n",
    "Let's define our first `SimpleRNN` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19a5fe-100a-4ecf-86f0-760aa5b4fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_rnn(input_shape, rnn_units=16):\n",
    "    # Input layer for sequences of shape (timesteps, features)\n",
    "    inputs = keras.Input(shape=input_shape, name='input_sequence')\n",
    "\n",
    "    # Simple RNN layer with ReLU activation\n",
    "    x = keras.layers.SimpleRNN(rnn_units, activation='relu')(inputs)\n",
    "\n",
    "    # Output layer for regression\n",
    "    outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=\"simple_rnn_weather_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af6324-159c-4b61-8d09-d1eaa94682f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_rnn = create_simple_rnn(input_shape=X_train.shape[1:], rnn_units=16)\n",
    "model_rnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5170124-7a51-4f3d-a976-abe954b28719",
   "metadata": {},
   "source": [
    "\n",
    "The SimpleRNN layer has three components contributing to its parameter count of 416\n",
    "\n",
    "- **Input weights (`U`)**:  \n",
    "  One weight per input feature × hidden unit  \n",
    "  → `9 input features × 16 units = 144`\n",
    "\n",
    "- **Recurrent weights (`V`)**:  \n",
    "  One weight per hidden unit × hidden unit  \n",
    "  → `16 units × 16 units = 256`\n",
    "\n",
    "  These weights connect the hidden state at time `t-1` to the hidden state at time `t`, enabling the model to \"remember\" and update internal state across timesteps.\n",
    "\n",
    "- **Biases (`b`)**:  \n",
    "  One bias term per hidden unit  \n",
    "  → `16`\n",
    "\n",
    "**Total for RNN**:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54f00f-d366-47ba-a0bb-1ac722ee7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "144 + 256 + 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cf288-6d1f-4c3e-869a-61ddc7eef463",
   "metadata": {},
   "source": [
    "These weights are **shared across all 30 timesteps** — the same weights are applied as the model \"unrolls\" across time. This is one of the reasons RNNs can model long sequences without requiring separate weights for each lag like a dense model would.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b501b-7ff5-42be-9c51-572af209666c",
   "metadata": {},
   "source": [
    "In comparison, our dense model containing 30 lags with all predictors had 32,201 weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e210a5a9-3d64-4c66-98f3-4a220aefaa43",
   "metadata": {},
   "source": [
    "### What does the Dense layer do here?\n",
    "After the RNN layer processes the full sequence, it returns the **final hidden state** — not the full sequence of outputs, just the one at the last timestep. This is typical in many-to-one sequence models (e.g., predicting tomorrow’s value from a 30-day window). If you want to output an entire sequence rather than just the next time-step, you can set: `return_sequences=True` in the RNN layer. We'll practice this in a few minutes.\n",
    "\n",
    "In the many-to-one setup we've used here, the RNN outputs a single vector of size 16 (one value per hidden unit). That vector is then passed to a `Dense` layer, which acts as the output layer. The Dense layer:\n",
    "\n",
    "- Takes all 16 values from the final hidden state\n",
    "- Learns one weight per unit, to map them to a single prediction (sunshine hours)\n",
    "- Adds a bias term\n",
    "\n",
    "So the 17 parameters in the Dense layer are:\n",
    "- **16 weights**: One from each hidden unit to the output\n",
    "- **1 bias**: A constant shift to the prediction\n",
    "\n",
    "**Note:** If you’ve seen diagrams like the one we showed above and below, you may notice an output weight matrix `W` that maps the hidden state `h_t` to an output `o_t` at each timestep. In our case, we aren’t producing an output at every step. Instead, we apply a single Dense layer *after* the final timestep, which effectively takes the place of `W` — but only once, not repeatedly. \n",
    "\n",
    "![Unrolled RNN](rnn_unfolded.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43249a-bd43-4cce-99af-3f0a104aadd8",
   "metadata": {},
   "source": [
    "### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f677e8e-ff99-4384-a90a-fa4e2560a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model(model_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8aabc-5626-4278-b8e9-2b6321b15d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_rnn = model_rnn.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=200,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[earlystopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec8ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history_rnn, ['root_mean_squared_error', 'val_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5314f9-94b7-475e-ae97-dde651f0a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8da8e1-c138-49f6-97ff-795c6ddb09b6",
   "metadata": {},
   "source": [
    "We see an improvement with the RNN model! Our test error is now 3.49 compared to the dense net's error of 3.96 (30 lags) and 3.59 (1 lag). Let's try a slighlty more complicated RNN. This time, we'll use two recurrent layers.\n",
    "\n",
    "#### RMSE's summarized\n",
    "* **RNN_30**: 3.49\n",
    "* **Dense_1**: 3.59\n",
    "* **Dense_30**: 3.96\n",
    "* **Baseline**: 4.55\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb278f-1454-469e-9f31-07cb25ffc83e",
   "metadata": {},
   "source": [
    "### Stacked RNN\n",
    "\n",
    "Let's try a stacked RNN next to give our model great capacity to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a18a4d-4449-4396-ba6c-eb1df96a183a",
   "metadata": {},
   "source": [
    "* The first SimpleRNN layer reads the input sequence and **returns a sequence of hidden states** — one for each timestep. It will have an output shape of: (batch_size, timesteps, units)\n",
    "This preserves temporal information across all steps.\n",
    "\n",
    "* The second SimpleRNN layer then treats that sequence as its input, processing it step by step and finally returning a single hidden state — the one from the final timestep.\n",
    "Output shape: (batch_size, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f46ab-f9aa-4296-b6fb-1148e66bfba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def create_stacked_rnn_model(input_shape, rnn_units=16):\n",
    "    inputs = keras.Input(shape=input_shape, name=\"input_sequence\")\n",
    "\n",
    "    x = keras.layers.SimpleRNN(rnn_units, return_sequences=True)(inputs) \n",
    "    x = keras.layers.SimpleRNN(rnn_units)(x) \n",
    "    outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"stacked_rnn_weather_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8120d5-8765-4fb2-b080-a6974dede9bf",
   "metadata": {},
   "source": [
    "### What happens when we stack recurrent layers?\n",
    "\n",
    "Stacking multiple RNN layers (e.g., using `return_sequences=True` in the first layer and feeding it into a second RNN) increases the **capacity** of the model — it allows the network to learn more complex or hierarchical temporal patterns.\n",
    "\n",
    "This is somewhat analogous to stacking convolutional layers: early layers learn local features, later layers learn broader context. In a stacked RNN:\n",
    "- The **first layer** processes the raw input sequence and emits a hidden state for each timestep.\n",
    "- The **second layer** sees that sequence of hidden states and learns higher-level patterns across time — for example, shifts in trends or repeated structures based on short-term signals.\n",
    "\n",
    "\n",
    "But there's a key limitation: stacking RNN layers doesn't fix their tendency to forget. Vanilla RNNs (like `SimpleRNN`) struggle to retain information over long sequences due to a problem called the **vanishing gradient problem**.\n",
    "\n",
    "During training, RNNs rely on backpropagation through time to update their weights. But as the number of timesteps increases, the gradient (the signal used to update weights) becomes smaller and smaller — until it's effectively zero. This means early inputs (within each input window) have little influence on model predictions, even if they were important.\n",
    "\n",
    "#### What counts as \"long\"?\n",
    "\n",
    "There’s no universal threshold, but **even moderately long sequences** — sometimes as short as 20–30 steps — can expose this limitation in practice. The tipping point depends on several factors:\n",
    "\n",
    "- The activation function (tanh exacerbates vanishing more than ReLU)\n",
    "- How weights are initialized\n",
    "- The complexity and variability of the sequence\n",
    "- The specifics of optimization and training dynamics\n",
    "\n",
    "In our case, with 30-day weather windows, SimpleRNN may already be at the edge of what it can retain reliably. As sequence length increases, the risk of forgetting early inputs becomes more severe unless mitigated.\n",
    "\n",
    "So while stacking adds modeling power, it doesn’t address this memory bottleneck.\n",
    "\n",
    "If your task requires learning long-range dependencies — for example, subtle signals from 30 days ago influencing tomorrow’s prediction — SimpleRNNs, even when stacked, are unlikely to perform well.\n",
    "\n",
    "In those cases, consider using:\n",
    "- **LSTM layers**: These use gating mechanisms to preserve and control memory over time\n",
    "- **Transformer-based models**: These use attention to model the entire sequence at once, avoiding recurrence altogether\n",
    "\n",
    "Stacked RNNs can be useful for capturing **short- to mid-range patterns**, but they’re not a solution for true long-term memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f1fa0-3f33-4cb9-9a21-d62a4d26ac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_rnn_stacked = create_stacked_rnn_model(input_shape=X_train.shape[1:], rnn_units=16)\n",
    "model_rnn_stacked.summary()\n",
    "compile_model(model_rnn_stacked)\n",
    "history_rnn_stacked = model_rnn_stacked.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[earlystopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb955fa-d996-4ace-a690-2785c6adf200",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_rnn_stacked, ['root_mean_squared_error', 'val_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e79efd-def5-4117-9c85-916ee592be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn_stacked.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba127b-c0aa-472c-ba4f-c1e394b10f1e",
   "metadata": {},
   "source": [
    "#### RMSE's summarized\n",
    "* **RNN_stacked_30**: 3.42\n",
    "* **RNN_30**: 3.49\n",
    "* **Dense_1**: 3.59\n",
    "* **Dense_30**: 3.96\n",
    "* **Baseline**: 4.55\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8121cf-4ebf-47f8-a2bf-9932be7c2e10",
   "metadata": {},
   "source": [
    "Further improvement! Yay recurrence! \n",
    "\n",
    "While we should certainly take a victory lap at this point, it may still be unsatistying that we're missing the mark by more than 3 hours, on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fad88",
   "metadata": {},
   "source": [
    "## The problem with vanilla RNNs\n",
    "\n",
    "Basic RNNs can capture short-term dependencies, but they struggle to retain information across long sequences — a limitation known as the vanishing gradient problem. Imagine whispering a message down a long chain of people like the game of telephone. As the message travels further, it degrades. This is similar to how gradient information gets lost in deep RNNs.\n",
    "\n",
    "This is why we need specialized architectures like **LSTM** that help preserve memory over long sequences.\n",
    "\n",
    "### LSTM to the rescue\n",
    "\n",
    "LSTM (Long Short-Term Memory) layers help address the vanishing gradient problem by adding a memory component: the cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9909a-3ff3-43f9-a053-5c3e642243d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#           ┌────────────┐\n",
    "# x_t ───►  │  LSTM cell │ ───►   h_t\n",
    "#           └────────────┘\n",
    "#             ▲       ▲\n",
    "#         h_{t-1}   c_{t-1} (memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701ac1d",
   "metadata": {},
   "source": [
    "At each timestep `t`, the LSTM takes:\n",
    "- the input `x_t`\n",
    "- the previous hidden state `h_{t-1}`\n",
    "- the previous cell state `c_{t-1}`\n",
    "\n",
    "The cell state acts as long-term memory, while the hidden state provides a short-term summary. The LSTM uses **gates with learned weights** to decide how much information to erase, update, or reveal at each step:\n",
    "\n",
    "- **Forget gate**: Learns which parts of the cell state to erase\n",
    "- **Input gate**: Learns which new information to add to memory\n",
    "- **Output gate**: Learns what information to send to the next hidden state\n",
    "\n",
    "These gates allow the model to maintain and control a persistent internal state across many timesteps, helping it overcome the vanishing gradient problem and track longer-term dependencies more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4b09d",
   "metadata": {},
   "source": [
    "## Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c92f2e-341d-473c-825f-6b73ec353172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, lstm_units=16):\n",
    "    inputs = keras.Input(shape=input_shape, name=\"input_sequence\")\n",
    "\n",
    "    # Stacked LSTM layers to compared to stacked RNN\n",
    "    x = keras.layers.LSTM(lstm_units, return_sequences=True)(inputs) \n",
    "    x = keras.layers.LSTM(lstm_units)(x) \n",
    "\n",
    "    # Output layer\n",
    "    outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"lstm_weather_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d8e5a-08c0-4bb7-9815-3c5d70b20a13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lstm = create_lstm_model(input_shape=X_train.shape[1:], lstm_units=7) # we'll choose a number of units so that the total param count is comparable to the stacked RNN (961)\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821fb114-0aec-4965-bdfd-04e5b16858ff",
   "metadata": {},
   "source": [
    "The LSTM layers each have four sets of parameters — one for each internal gate (input, forget, cell, output). Each gate has its own set of input weights, recurrent weights, and a bias term. That’s why the total parameter count for each LSTM layer scales with a factor of 4.\n",
    "\n",
    "These parameters are shared across all timesteps in the input sequence, so the number of parameters depends on the input/output dimensions — not the sequence length.\n",
    "\n",
    "### Breakdown of parameters\n",
    "\n",
    "#### First LSTM layer (`lstm`)\n",
    "- **Input shape**: (batch_size, 30 timesteps, 9 features)\n",
    "- **Output shape**: (batch_size, 30 timesteps, 7 units)\n",
    "- **Parameters**: 476\n",
    "\n",
    "Breakdown:\n",
    "- Input weights: `9 inputs × 7 units × 4 gates = 252`\n",
    "- Recurrent weights: `7 units × 7 units × 4 gates = 196`\n",
    "- Biases: `7 units × 4 gates = 28`\n",
    "- **Total**: 252 + 196 + 28 = **476**\n",
    "\n",
    "#### Second LSTM layer (`lstm_1`)\n",
    "- **Input shape**: (batch_size, 30 timesteps, 7 features from previous LSTM)\n",
    "- **Output shape**: (batch_size, 7 units)\n",
    "- **Parameters**: 420\n",
    "\n",
    "Breakdown:\n",
    "- Input weights: `7 inputs × 7 units × 4 gates = 196`\n",
    "- Recurrent weights: `7 units × 7 units × 4 gates = 196`\n",
    "- Biases: `7 units × 4 gates = 28`\n",
    "- **Total**: 196 + 196 + 28 = **420**\n",
    "\n",
    "#### Final Dense layer\n",
    "- Input shape: 7 (from final LSTM output)\n",
    "- Output shape: 1\n",
    "- **Parameters**: 7 weights + 1 bias = **8**\n",
    "\n",
    "**Total model parameters**:  \n",
    "476 (first LSTM) + 420 (second LSTM) + 8 (Dense) = **904**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ee2f6-149f-47a8-8bfe-c1b2a71cdcdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compile_model(model_lstm)\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=200,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[earlystopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48097311-c34e-4e13-85ee-c22fbf4bbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_lstm, ['root_mean_squared_error', 'val_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ef013-cd57-412c-a5f2-05105a1173cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99e87c-ffd7-4ad4-90af-2567a9e7f43e",
   "metadata": {},
   "source": [
    "#### RMSE's summarized\n",
    "* **LSTM_stacked_30**: 3.48\n",
    "* **RNN_stacked_30**: 3.42\n",
    "* **RNN_30**: 3.49\n",
    "* **Dense_1**: 3.59\n",
    "* **Dense_30**: 3.96\n",
    "* **Baseline**: 4.55\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc688923-5550-4dad-b41d-2b770b8b979a",
   "metadata": {},
   "source": [
    "### Discuss\n",
    "Why might the LSTM be doing worse in this situation? What do you think will happen if we increase the sequence length?\n",
    "\n",
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edfe50-897f-4ef8-96e7-348bfb398a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "window_size_larger = 90\n",
    "\n",
    "# Build sequences\n",
    "n_seq = len(data_basel) - window_size_larger\n",
    "\n",
    "X_seq = np.stack([\n",
    "    data_basel[basel_columns].iloc[i:i+window_size_larger].values\n",
    "    for i in range(n_seq)\n",
    "])\n",
    "print(\"X_seq shape:\", X_seq.shape)  # (samples, 30, features)\n",
    "\n",
    "y_seq = data_basel['BASEL_sunshine'].iloc[window_size_larger:].values\n",
    "\n",
    "print(\"y_seq shape:\", y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63556c58-67c9-4129-84c1-c65abdeb2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X_seq, y_seq, test_size=test_set_size, random_state=0, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=0, shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5b972-2bca-4efb-bee1-442045309c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lstm = create_lstm_model(input_shape=X_train.shape[1:], lstm_units=7)\n",
    "model_lstm.summary()\n",
    "compile_model(model_lstm)\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=200,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[earlystopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684330c-88eb-49e3-8756-a89b220ddf26",
   "metadata": {},
   "source": [
    "**Note**: You may notice that training takes a while. RNNs and LSTMs are notoriously very slow to train because they processs data one input at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528685e-1c18-4661-97e5-d90bd833b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_lstm, ['root_mean_squared_error', 'val_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15138eb-0586-498d-8231-08bbbdc5f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cc10c-1823-463b-baea-543171066022",
   "metadata": {},
   "source": [
    "#### RMSE's summarized\n",
    "* **LSTM_stacked_90**: 3.43\n",
    "* **LSTM_stacked_30**: 3.47\n",
    "* **RNN_stacked_30**: 3.42\n",
    "* **RNN_30**: 3.49\n",
    "* **Dense_1**: 3.59\n",
    "* **Dense_30**: 3.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7ed0a-4a07-4e77-a8c7-c308161aa450",
   "metadata": {},
   "source": [
    "### Discuss result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb7989-14de-442e-8a43-0b0f1a0185ba",
   "metadata": {},
   "source": [
    "### Exercise: Adding dropout\n",
    "\n",
    "Dropout is a common regularization technique that helps prevent overfitting by randomly \"dropping\" units during training. However, **how you apply dropout depends on the type of layer**.\n",
    "\n",
    "- For **dense layers**, you add a separate `Dropout(...)` layer in between the dense layers.\n",
    "- For **RNNs and LSTMs**, you don't use a separate layer. Instead, you pass dropout settings directly as arguments to the recurrent layer itself. This ensures that dropout is applied correctly over time without disrupting the sequence structure.\n",
    "\n",
    "In this exercise, you'll experiment with adding dropout to the models we built earlier. You may want to create a copy of this notebook for this next experiment so you don't lose track of our current results.\n",
    "\n",
    "#### 1. Add dropout of 0.1 to the RNN and LSTM models (both 30 and 90 window sizes)\n",
    "\n",
    "Use the built-in dropout arguments:\n",
    "```python\n",
    "dropout = 0.1 # add this at top of notebook in case you want to experiment with other values\n",
    "# For SimpleRNN\n",
    "x = keras.layers.SimpleRNN(rnn_units, activation='relu', dropout=dropout)(inputs)\n",
    "\n",
    "# For LSTM\n",
    "x = keras.layers.LSTM(lstm_units, dropout=dropout)(x)\n",
    "\n",
    "```\n",
    "\n",
    "Run each model with and without dropout, and compare their performance.\n",
    "Does dropout help reduce overfitting? How does it affect validation and test RMSE?\n",
    "\n",
    "#### 2. Add dropout to the dense model\n",
    "For dense layers, add Dropout as a separate layer after the first activation:\n",
    "\n",
    "```python\n",
    "x = keras.layers.Dense(100, activation='relu')(inputs)\n",
    "x = keras.layers.Dropout(dropout)(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452ab97-7b35-4b5b-bdac-7adf57200f4a",
   "metadata": {},
   "source": [
    "### Discuss: Validation vs test \n",
    "If we are still comparing models at this stage, should we be using the test set to compare performance?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb88f7cb-3093-4144-ba48-3bf45d6638be",
   "metadata": {},
   "source": [
    "### Bonus exercises as homework\n",
    "\n",
    "1. Use keras' tuner to find a decent set of hyperparameters for each model tested (RNN, LSTM and Dense). Experiment with different number of units, activation functoins, and dropout levels. Feel free to experiement with others if you wish, but be mindful of combinatorial explosion. \n",
    "2. Adjust window_size to 3 (near the top of this notebook), and restart the notebook kernel / run all cells. Which model performs the best? Why do you think this might be?\n",
    "3. Adjust window_size to 360 (near the top of this notebook), and restart the notebook kernel / run all cells. Which model performs the best? Why do you think this might be?\n",
    "4. Adjust the patience parameter to 50, and max epochs to 10000. This should help us find the \"second descent\" in the training curve, if one is possible to observe with this data. How do the training curves look? How long did training take? Were you able to find a better model?\n",
    "5. Try to fit a CNN to the data. Does it do any better than the LSTM?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba5536",
   "metadata": {},
   "source": [
    "## Keypoints\n",
    "\n",
    "- RNNs and LSTMs allow neural networks to process data step-by-step\n",
    "- LSTMs retain long-term context using gated memory\n",
    "- Sequence models are widely used in time series, language, and biology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c29d7",
   "metadata": {},
   "source": [
    "### Wrapping up: What is attention?\n",
    "\n",
    "So far, we’ve seen how RNNs and LSTMs process sequences step by step and pass a hidden state forward in time as a kind of memory. But ultimately, they compress everything they’ve seen into a single vector — often just the final hidden state — and use that to make a prediction.\n",
    "\n",
    "That's a limitation when important information appears earlier in the sequence. Even with gates, LSTMs can struggle to retain distant context.\n",
    "\n",
    "Attention changes this by asking a different question: Instead of trying to remember everything as you go, why not look back across all the memories you built — and decide which ones matter most right now?\n",
    "\n",
    "You can think of attention as a spotlight the model moves across the sequence of past hidden states. It assigns weights to each timestep based on how relevant it thinks that step is for the current task. Then it combines those weighted memories into a new context vector, which it uses to make a prediction.\n",
    "\n",
    "This allows the model to:\n",
    "- Access the full sequence without compressing it into a single state\n",
    "- Focus on different parts of the sequence depending on the context\n",
    "- Better handle long-range dependencies\n",
    "\n",
    "Attention was originally introduced alongside RNNs and LSTMs — for tasks like translation, where aligning parts of an input with an output matters. But later, researchers asked: what if we removed recurrence altogether and just used attention? That led to the Transformer architecture.\n",
    "\n",
    "We'll come back to Transformers in a future lesson. For now, just keep this in mind:\n",
    "\n",
    "LSTMs decide what to remember over time.  \n",
    "Attention decides where to look across the stored memories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882af60d-5426-42b3-b852-7ff1d1e05654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
