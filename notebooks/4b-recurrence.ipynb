{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d25247",
   "metadata": {},
   "source": [
    "# Advanced layer types: Recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94fefc",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- Why do we need layers specifically designed for sequential data?\n",
    "- What are Recurrent Neural Networks (RNNs) and LSTMs?\n",
    "- How does an LSTM \"remember” important information over time?\n",
    "- What are alternatives like attention?\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand the structure and motivation behind RNN and LSTM layers\n",
    "- Relate LSTM concepts to earlier architectures (dense, CNN)\n",
    "- Explore a simple forecasting example using LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b362b5d-6dae-4e21-8062-8e33253f0c81",
   "metadata": {},
   "source": [
    "## Revisiting sunshine hours\n",
    "\n",
    "Yesterday, we predicted today's sunshine hours (in Basel) using weather variables from just yesterday — a one-to-one mapping. Each input was a single day's data. Let's rebuild that model quickly to remind ourselves of the test set performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94eca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"https://zenodo.org/record/5071376/files/weather_prediction_dataset_light.csv?download=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca05218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename_data = \"data/weather_prediction_dataset_light.csv\"\n",
    "data = pd.read_csv(filename_data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cd7fa-5adf-44cf-97ef-17fdcf6386c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only Basel-specific predictors\n",
    "basel_columns = [col for col in data.columns if col.startswith('BASEL_')]\n",
    "\n",
    "# Define number of rows (e.g., 9 years of daily data)\n",
    "nr_rows = 365 * 9\n",
    "\n",
    "# Drop DATE and MONTH, keep Basel predictors\n",
    "X_data = data.loc[:nr_rows, basel_columns]\n",
    "y_data = data.loc[1:(nr_rows + 1), \"BASEL_sunshine\"]\n",
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae69856-327a-4af7-9883-a1003415b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099059f5-cd22-4738-b92a-c3020bde26bc",
   "metadata": {},
   "source": [
    "This time, we'll apply a more appropriate split for a temporal dataset. We'll turn off shuffling so that the test set consists only of later time points than those in the training set. This setup allows us to evaluate how well the model can predict future values using only past information — without accidentally training on data from the future. This is important, because, the future often doesn't resemble the past, and we want to see how well our model can handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab94c7-f53f-4ab4-a1f1-4159929fe5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_set_size = 0.2\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X_data, y_data, test_size=test_set_size, random_state=0, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=0, shuffle=False)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d24190-cfd6-4fce-b2b4-ed4a5d72d697",
   "metadata": {},
   "source": [
    "Set seeds to control for random weight initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff174c96-f7d8-4daf-a2a1-86041c68d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0a964-36bc-41c8-8eea-700d0b786547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def create_dense_nn(input_shape):\n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=input_shape, name='input')\n",
    "\n",
    "    # Dense layers\n",
    "    layers_dense = keras.layers.Dense(100, 'relu')(inputs)\n",
    "    layers_dense = keras.layers.Dense(50, 'relu')(layers_dense)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = keras.layers.Dense(1)(layers_dense)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=\"dense_weather_prediction_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14edfb-6694-4c7f-b26f-4ecee50ae171",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense = create_dense_nn(input_shape=(X_data.shape[1],))\n",
    "model_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c3415-9f3f-4490-893f-8ec1cc5ce48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse',\n",
    "                  metrics=[keras.metrics.RootMeanSquaredError()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8d77c-e184-4040-9be5-e475eb56798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model(model_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725dc763-771b-415c-aa14-720966e8b9e2",
   "metadata": {},
   "source": [
    "Fit with early stopping, as we did previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6930f70-f1e3-420f-87f7-7a998e6b4f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystopper = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10\n",
    "    )\n",
    "\n",
    "history_dense = model_dense.fit(X_train, y_train,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 200,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8efbee-5d0d-4ed0-a22e-4cd6c8061ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history, metrics):\n",
    "    \"\"\"\n",
    "    Plot the training history\n",
    "\n",
    "    Args:\n",
    "        history (keras History object that is returned by model.fit())\n",
    "        metrics (str, list): Metric or a list of metrics to plot\n",
    "    \"\"\"\n",
    "    history_df = pd.DataFrame.from_dict(history.history)\n",
    "    sns.lineplot(data=history_df[metrics])\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"metric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aab70f-4a2b-4caf-afdd-3c95f3227db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_dense, ['root_mean_squared_error', 'val_root_mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112f2d4-2e06-48a9-9a9d-b1e85df55236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcee0a-5717-456f-8709-c65eeba7d6eb",
   "metadata": {},
   "source": [
    "If you recall, our baseline model had an RMSE of 3.88. Our fully connect neural network only does slightly better than baseline (3.59). While it's encouraging to see an improvement to the baseline, there is much more we can do to improve this result. \n",
    "\n",
    "Recall that here, we are only looking at the current day's weather (across cities) to determine sunshine hours in Basel the next day. But what if sunshine patterns depend on the past week, past month, or past year? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc3ea1-6120-4ae6-919f-8a88912145fe",
   "metadata": {},
   "source": [
    "### When is a single lag not enough?\n",
    "\n",
    "In many real-world tasks, yesterday's data alone isn't sufficient — patterns unfold over time:\n",
    "\n",
    "- Rainy streaks often last several days\n",
    "- Cold fronts move gradually, not all at once\n",
    "- In other domains: heartbeats, language, gestures, and biological sequences like proteins rely on order and context across multiple steps\n",
    "\n",
    "#### Include mutliple lags manually?\n",
    "A natural next step is to **include multiple lags manually** as input features. \n",
    "\n",
    "For example: add sunshine hours from the last 30 days as separate columns. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942dd14-3c3f-42e4-8406-86c566527fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create lagged features for all Basel-specific predictors\n",
    "data_lagged = data.copy()\n",
    "basel_columns = [col for col in data.columns if col.startswith('BASEL_')]\n",
    "\n",
    "# Add lags for each Basel predictor (store in dictionary first)\n",
    "window_size = 30\n",
    "lagged_features = {}\n",
    "\n",
    "for col in basel_columns:\n",
    "    for lag in range(1, window_size + 1):\n",
    "        lagged_features[f'{col}_lag{lag}'] = data[col].shift(lag)\n",
    "\n",
    "# Concatenate all lagged features at once\n",
    "data_lagged = pd.concat([data_lagged, pd.DataFrame(lagged_features)], axis=1)\n",
    "\n",
    "# Drop rows with NaNs caused by lagging\n",
    "data_lagged = data_lagged.dropna().reset_index(drop=True)\n",
    "\n",
    "# Define X and y using only lagged Basel features\n",
    "all_lagged_cols = [col for col in data_lagged.columns if col.startswith('BASEL_') and 'lag' in col]\n",
    "X_data_lagged = data_lagged.loc[:nr_rows, all_lagged_cols]\n",
    "y_data_lagged = data_lagged.loc[:nr_rows, 'BASEL_sunshine']  # target is same as before\n",
    "\n",
    "# Train/validation/test split (preserving time order)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_lagged, X_holdout_lagged, y_train_lagged, y_holdout_lagged = train_test_split(\n",
    "    X_data_lagged, y_data_lagged, test_size=test_set_size, random_state=0, shuffle=False)\n",
    "\n",
    "X_val_lagged, X_test_lagged, y_val_lagged, y_test_lagged = train_test_split(\n",
    "    X_holdout_lagged, y_holdout_lagged, test_size=0.5, random_state=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2c8c5-86e3-4ff0-918f-8463b71bdf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lagged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb844f39-2961-4013-878e-229d1d9c89a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_lagged.shape)\n",
    "print(X_test_lagged.shape)\n",
    "print(X_val_lagged.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704f59a-be21-47f7-85a8-0ba2bcf46140",
   "metadata": {},
   "source": [
    "**Note**: Number of predictors grows drastically! How do you think this will impact model performance? Let's try it and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68fb19-1d66-4b1f-abe8-93ce3f3d15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dense model with additional input features\n",
    "model_dense_lagged = create_dense_nn(input_shape=(X_train_lagged.shape[1],))\n",
    "\n",
    "# view model summary\n",
    "model_dense_lagged.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb704cb9-725b-4764-9591-e011cc481972",
   "metadata": {},
   "source": [
    "**Note**: With 270 input features, the total number of weights increases dramaticaly as well. 32,301 (30 lags) >> 6,101 (1 lag)\n",
    "\n",
    "Compile and train the model next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47450062-5545-40de-b005-3b1238506928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model \n",
    "compile_model(model_dense_lagged)\n",
    "# train model and store results\n",
    "history_dense_lagged = model_dense_lagged.fit(X_train_lagged, y_train_lagged,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 200,\n",
    "                    validation_data=(X_val_lagged, y_val_lagged),\n",
    "                    callbacks=[earlystopper])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d67fe-e321-403e-a978-cb3a95236498",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_dense_lagged, ['root_mean_squared_error', 'val_root_mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6ea63-9347-4b6b-b6dd-f0db487d8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense_lagged.evaluate(X_test_lagged, y_test_lagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d39741-9fd2-454e-8766-e00269f587e4",
   "metadata": {},
   "source": [
    "Here, we see that the explosion in input parameters has made the problem more challenging to model. We get a test set error of 3.96 compared to 3.59 in our previous dense model.\n",
    "\n",
    "### Discuss: What might be happening here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f461cea-cb18-40ec-aaa0-2fd8d57afa44",
   "metadata": {},
   "source": [
    "### Why manual lagging isn’t enough\n",
    "\n",
    "To give our models access to past information, we previously added lagged versions of each predictor. This approach works — but it has several limitations:\n",
    "\n",
    "- **No sense of time**: The model treats lagged inputs as unordered. It doesn’t know that lag 1 is closer to the present than lag 30.\n",
    "- **Feature explosion**: The number of input features grows quickly with the window size. With 30 lags across 9 features, we ended up with 270 inputs.\n",
    "- **Manual design choices**: We have to choose which lags and predictors to include, which introduces rigidity and may not generalize well.\n",
    "- **Separate weights for each lag**: A dense model learns different weights for each timestep, making it hard to capture shifting or repeating patterns over time.\n",
    "\n",
    "In short, manually lagging inputs increases model complexity and overfitting risk without offering the model a structured way to reason about time.\n",
    "\n",
    "### Introducing recurrence\n",
    "\n",
    "Recurrent neural networks (RNNs) offer a more efficient way to model temporal data. Instead of flattening the past into a wide array of lagged features, RNNs process sequences step by step — maintaining a **hidden state** that evolves over time.\n",
    "\n",
    "At each timestep `t`, the RNN sees:\n",
    "- the current input `x_t`, and\n",
    "- the previous hidden state `h_{t-1}`, which summarizes all earlier inputs\n",
    "\n",
    "This update looks like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad256db1-cbe0-4af0-bb07-718cc1587b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#           ┌────────────┐\n",
    "# x_t ───►  │  RNN cell  │ ───►  h_t\n",
    "#           └────────────┘\n",
    "#             ▲       \n",
    "#         h_{t-1}     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694cf335-4b9e-4e6c-a9f7-70d8723de954",
   "metadata": {},
   "source": [
    "### Why RNNs don't need lagged features\n",
    "You can think of the hidden state h_(t) in an RNN as a compressed, learned summary or memory of everything the model has seen so far — not just one specific lag like x_{t-1}, but a running representation of the entire sequence up to that point. RNNs effectively create a *learned memory* of the past. They resemble a first-order Markov process in that each hidden state depends only on the previous one — but unlike traditional Markov models, the state is learned and far more expressive.\n",
    "\n",
    "This shared-memory approach:\n",
    "- makes the model more efficient: the **same weights** are used at every timestep, keeping parameter count low\n",
    "- reduces the risk of overfitting, and\n",
    "- the model doesn't need to be handed 30 explicit lags — it **learns what to remember** over time.\n",
    "- allows model to flexibly capture patterns at multiple time scales.\n",
    "\n",
    "In short: instead of manually handing over a huge chunk of history, we let the model carry and refine a compact representation of the past over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf939b1d-5ef5-40b4-be04-d0c903ef4a45",
   "metadata": {},
   "source": [
    "### A simple recurrent model\n",
    "Recurrent layers expect 3D inputs: one sequence per sample. Instead of flattening the window (30 days) into 1 wide row, we reshape our data into a sequence of timesteps, each with a vector of predictors. At each time step t, the RNN takes:\n",
    "- the input at this step x_t\n",
    "- the hidden state from only the previous step h_{t-1}\n",
    "\n",
    "It produces a new hidden state h_t, which is used for the next time step and sometimes for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe384a91-315e-4912-94df-92d3cc3d61b4",
   "metadata": {},
   "source": [
    "#### Preparing the inputs and outputs\n",
    "To use a recurrent model, we'll restructure our dataset so that each sample is a short sequence of consecutive days — each with the same set of predictors. Recurrent layers expect 3D input tensors:  \n",
    "**(samples, timesteps, features per timestep)**\n",
    "\n",
    "We'll start from the original data and build these sequences explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c884d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = data.loc[:nr_rows].drop(columns=[\"DATE\", \"MONTH\"]) # nr_rows gives us 3 years of data again\n",
    "y_data = data.loc[window_size:(nr_rows + window_size)][\"BASEL_sunshine\"] # predict starting with 8th day\n",
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d949776-9f5d-4d21-b031-8c389f026293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only BASEL predictors \n",
    "basel_columns = [col for col in data.columns if col.startswith('BASEL_')]\n",
    "\n",
    "# Drop NaNs and reset index to keep it tidy\n",
    "data_basel = data[basel_columns].dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df8de0-460b-4f23-a35d-7bd347ba884a",
   "metadata": {},
   "source": [
    "#### How are the input sequences and targets constructed?\n",
    "\n",
    "Each input sample is a sequence of 30 consecutive days of predictor values.  \n",
    "We slide this 30-day window over the dataset to create many such sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb014a8-0ada-400f-a1f6-fc84465493b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Build sequences\n",
    "n_seq = len(data_basel) - window_size\n",
    "\n",
    "X_seq = np.stack([\n",
    "    data_basel[basel_columns].iloc[i:i+window_size].values\n",
    "    for i in range(n_seq)\n",
    "])\n",
    "print(\"X_seq shape:\", X_seq.shape)  # (samples, 30, features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2158f-fe9b-43b1-9a0f-e3fbcc54116e",
   "metadata": {},
   "source": [
    "This creates a 3D array of shape (samples, timesteps=30, features). Each slice `X_seq[i]` contains predictors for days i through i+window_size\n",
    "\n",
    "For each input sequence, we want the model to predict the sunshine on the day after the last timestep (i.e., day i+window_size). We align the target values accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bbe71-0963-43ce-9947-35a8f449d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_seq = data_basel['BASEL_sunshine'].iloc[window_size:].values\n",
    "\n",
    "print(\"y_seq shape:\", y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X_seq, y_seq, test_size=test_set_size, random_state=0, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=0, shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a108a6bd-3b2c-4072-aff3-77d4d6885332",
   "metadata": {},
   "source": [
    "### Modeling sequences with a recurrent neural network\n",
    "\n",
    "Now that our input data is structured as sequences (30 timesteps × 89 features), we can use a recurrent neural network to learn from patterns over time.\n",
    "\n",
    "Instead of manually adding lagged features, we feed the model the full sequence and let it learn which parts of the past are useful. This keeps the input compact while enabling the model to capture temporal dependencies internally.\n",
    "\n",
    "Let's define our first `SimpleRNN` model using the same Keras Functional API as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19a5fe-100a-4ecf-86f0-760aa5b4fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_simple_rnn(input_shape, rnn_units=16):\n",
    "    # Input layer for sequences of shape (timesteps, features)\n",
    "    inputs = keras.Input(shape=input_shape, name='input_sequence')\n",
    "\n",
    "    # Simple RNN layer with ReLU activation\n",
    "    x = keras.layers.SimpleRNN(rnn_units, activation='relu')(inputs)\n",
    "\n",
    "    # Output layer for regression\n",
    "    outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=\"simple_rnn_weather_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af6324-159c-4b61-8d09-d1eaa94682f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_rnn = create_simple_rnn(input_shape=X_train.shape[1:], rnn_units=16)\n",
    "model_rnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5170124-7a51-4f3d-a976-abe954b28719",
   "metadata": {},
   "source": [
    "\n",
    "The SimpleRNN layer has three components contributing to its parameter count of 416\n",
    "\n",
    "- **Input weights (`W`)**:  \n",
    "  One weight per input feature × hidden unit  \n",
    "  → `9 input features × 16 units = 144`\n",
    "\n",
    "- **Recurrent weights (`U`)**:  \n",
    "  One weight per hidden unit × hidden unit  \n",
    "  → `16 units × 16 units = 256`\n",
    "\n",
    "  These weights connect the hidden state at time `t-1` to the hidden state at time `t`, enabling the model to \"remember\" and update internal state across timesteps.\n",
    "\n",
    "- **Biases (`b`)**:  \n",
    "  One bias term per hidden unit  \n",
    "  → `16`\n",
    "\n",
    "**Total for RNN**:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54f00f-d366-47ba-a0bb-1ac722ee7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "144 + 256 + 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cf288-6d1f-4c3e-869a-61ddc7eef463",
   "metadata": {},
   "source": [
    "These weights are **shared across all 30 timesteps** — the same weights are applied as the model \"unrolls\" across time. This is one of the reasons RNNs can model long sequences without requiring separate weights for each lag like a dense model would.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b501b-7ff5-42be-9c51-572af209666c",
   "metadata": {},
   "source": [
    "In comparison, our dense model containing 30 lags with all predictors had 32,201 weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa22b0e-05e1-4aed-8691-dfb618453561",
   "metadata": {},
   "source": [
    "### What it means to \"share weights\" in an RNN\n",
    "\n",
    "In a standard feedforward neural net, each feature connects to a unique set of weights. When we added lagged features, that meant each lag had its own distinct weights.\n",
    "\n",
    "But in an RNN, we loop through time — and at each step, the same weight matrices are used:\n",
    "- One set connects the input at time `t` to the hidden state \n",
    "- Another connects the previous hidden state to the next \n",
    "- A bias vector is added (`b`)\n",
    "\n",
    "These shared weights are trained to work across *all* timesteps. That’s what allows an RNN to model temporal dependencies **without needing a separate set of weights for each lag**.\n",
    "\n",
    "This is why RNNs are often more parameter-efficient than wide dense models that treat time as flat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e210a5a9-3d64-4c66-98f3-4a220aefaa43",
   "metadata": {},
   "source": [
    "### What does the Dense layer do here?\n",
    "\n",
    "After the RNN processes the full sequence, it returns the **final hidden state** — not the full sequence of outputs, just the one at the **last timestep**. This is a single vector of size 32 (one value per hidden unit).\n",
    "\n",
    "That vector is then passed to the `Dense` layer, which:\n",
    "\n",
    "- Takes all 16 hidden units from the final timestep\n",
    "- Learns one weight per unit, to map them to a single prediction (sunshine hours)\n",
    "- Adds a bias term\n",
    "\n",
    "So the 17 parameters in the dense layer are:\n",
    "\n",
    "- **16 weights**: One from each hidden unit to the output\n",
    "- **1 bias**: Shifts the final output up or down\n",
    "\n",
    " Even though the RNN saw `window_size` days of data, **only the final hidden state is passed forward** to make the prediction. This is typical in many-to-one sequence models (e.g. predict tomorrow from the last 30 days).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43249a-bd43-4cce-99af-3f0a104aadd8",
   "metadata": {},
   "source": [
    "### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f677e8e-ff99-4384-a90a-fa4e2560a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model(model_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8aabc-5626-4278-b8e9-2b6321b15d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_rnn = model_rnn.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=200,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[earlystopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec8ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history_rnn, ['root_mean_squared_error', 'val_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5314f9-94b7-475e-ae97-dde651f0a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8da8e1-c138-49f6-97ff-795c6ddb09b6",
   "metadata": {},
   "source": [
    "We see an improvement with the RNN model! Our test error is now 3.487 compared to the dense net's error of 3.96. Let's try a slighlty more complicated RNN. This time, we'll use two recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a18a4d-4449-4396-ba6c-eb1df96a183a",
   "metadata": {},
   "source": [
    "The first SimpleRNN layer reads the input sequence and returns a sequence of hidden states — one for each timestep -> Output shape: (batch_size, timesteps, units)\n",
    "This preserves temporal information across all steps.\n",
    "\n",
    "The second SimpleRNN layer then treats that sequence as its input, processing it step by step and finally returning a single hidden state — the one from the final timestep.\n",
    "Output shape: (batch_size, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f46ab-f9aa-4296-b6fb-1148e66bfba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def create_stacked_rnn_model(input_shape, rnn_units=16):\n",
    "    inputs = keras.Input(shape=input_shape, name=\"input_sequence\")\n",
    "\n",
    "    x = keras.layers.SimpleRNN(rnn_units, return_sequences=True)(inputs) \n",
    "    x = keras.layers.SimpleRNN(rnn_units)(x) \n",
    "    outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"stacked_rnn_weather_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f1fa0-3f33-4cb9-9a21-d62a4d26ac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_rnn_stacked = create_stacked_rnn_model(input_shape=X_train.shape[1:], rnn_units=16)\n",
    "model_rnn_stacked.summary()\n",
    "compile_model(model_rnn_stacked)\n",
    "history_rnn_stacked = model_rnn_stacked.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[earlystopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb955fa-d996-4ace-a690-2785c6adf200",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_rnn_stacked, ['root_mean_squared_error', 'val_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e79efd-def5-4117-9c85-916ee592be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn_stacked.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8121cf-4ebf-47f8-a2bf-9932be7c2e10",
   "metadata": {},
   "source": [
    "Further improvement! Yay recurrence!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fad88",
   "metadata": {},
   "source": [
    "## The problem with vanilla RNNs\n",
    "\n",
    "Basic RNNs can capture short-term dependencies, but they struggle to retain information across long sequences — a limitation known as the vanishing gradient problem.\n",
    "\n",
    "Imagine trying to predict the next word in a sentence:\n",
    "\n",
    "> I grew up in France… I speak fluent ___.\n",
    "\n",
    "You want the model to remember \"France\" — even if it happened many steps earlier. Vanilla RNNs often forget these long-range dependencies.\n",
    "\n",
    "\n",
    "\n",
    "## LSTM to the rescue\n",
    "\n",
    "LSTM (Long Short-Term Memory) layers address this by adding a memory component: the cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9909a-3ff3-43f9-a053-5c3e642243d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#           ┌────────────┐\n",
    "# x_t ───►  │  LSTM cell │ ───►   h_t\n",
    "#           └────────────┘\n",
    "#             ▲       ▲\n",
    "#         h_{t-1}   c_{t-1} (memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701ac1d",
   "metadata": {},
   "source": [
    "At each time step t, the LSTM takes:\n",
    "- the input x_t\n",
    "- the previous hidden state h_{t-1}\n",
    "- the previous cell state c_{t-1}\n",
    "\n",
    "The cell state acts as long-term memory, while the hidden state provides a short-term summary. Gates inside the LSTM control how much information to forget, store, or expose.\n",
    "\n",
    "- **Forget gate**: What information should be erased from memory?\n",
    "- **Input gate**: What new information should be stored?\n",
    "- **Output gate**: What part of the memory should be passed forward?\n",
    "\n",
    "This lets the model maintain a persistent internal state across many steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4b09d",
   "metadata": {},
   "source": [
    "Train a basic LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c92f2e-341d-473c-825f-6b73ec353172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, lstm_units=16):\n",
    "    inputs = keras.Input(shape=input_shape, name=\"input_sequence\")\n",
    "\n",
    "    # Stacked LSTM layers to compared to stacked RNN\n",
    "    x = keras.layers.LSTM(lstm_units, return_sequences=True)(inputs) \n",
    "    x = keras.layers.LSTM(lstm_units)(x) \n",
    "\n",
    "    # Output layer\n",
    "    outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"lstm_weather_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d8e5a-08c0-4bb7-9815-3c5d70b20a13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lstm = create_lstm_model(input_shape=X_train.shape[1:], lstm_units=16)\n",
    "model_lstm.summary()\n",
    "compile_model(model_lstm)\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[earlystopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821fb114-0aec-4965-bdfd-04e5b16858ff",
   "metadata": {},
   "source": [
    "The first LSTM layer (`lstm_7`) has four sets of parameters, one for each of its internal gates (input, forget, cell, output). Each set includes input weights, recurrent weights, and a bias term — so the total parameter count is scaled by a factor of 4.\n",
    "\n",
    "### Breakdown of parameters in `lstm_7` (1,664 total):\n",
    "\n",
    "- **Input weights (`W`)**:  \n",
    "  One weight per input feature × hidden unit × 4 gates  \n",
    "  → `9 input features × 16 units × 4 = 576`\n",
    "\n",
    "- **Recurrent weights (`U`)**:  \n",
    "  One weight per hidden unit × hidden unit × 4 gates  \n",
    "  → `16 units × 16 units × 4 = 1,024`\n",
    "\n",
    "- **Biases (`b`)**:  \n",
    "  One bias term per unit × 4 gates  \n",
    "  → `16 units × 4 = 64`\n",
    "\n",
    "These weights are **shared across all timesteps** in the input sequence, which helps the model learn how to update its memory and output at each time step without growing the parameter count linearly with the sequence length.\n",
    "\n",
    "**Total for `lstm_7`**:  \n",
    "576 (input weights) + 1,024 (recurrent weights) + 64 (biases) = **1,664 parameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48097311-c34e-4e13-85ee-c22fbf4bbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_lstm, ['root_mean_squared_error', 'val_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ef013-cd57-412c-a5f2-05105a1173cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc688923-5550-4dad-b41d-2b770b8b979a",
   "metadata": {},
   "source": [
    "Let's try a larger window length of 90 days. The LSTM model should be able to capture longer-range dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edfe50-897f-4ef8-96e7-348bfb398a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "window_size_larger = 90\n",
    "\n",
    "# Build sequences\n",
    "n_seq = len(data_basel) - window_size_larger\n",
    "\n",
    "X_seq = np.stack([\n",
    "    data_basel[basel_columns].iloc[i:i+window_size_larger].values\n",
    "    for i in range(n_seq)\n",
    "])\n",
    "print(\"X_seq shape:\", X_seq.shape)  # (samples, 30, features)\n",
    "\n",
    "y_seq = data_basel['BASEL_sunshine'].iloc[window_size_larger:].values\n",
    "\n",
    "print(\"y_seq shape:\", y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63556c58-67c9-4129-84c1-c65abdeb2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X_seq, y_seq, test_size=test_set_size, random_state=0, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=0, shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5b972-2bca-4efb-bee1-442045309c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = create_lstm_model(input_shape=X_train.shape[1:], lstm_units=16)\n",
    "model_lstm.summary()\n",
    "compile_model(model_lstm)\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[earlystopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528685e-1c18-4661-97e5-d90bd833b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_lstm, ['root_mean_squared_error', 'val_root_mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15138eb-0586-498d-8231-08bbbdc5f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba5536",
   "metadata": {},
   "source": [
    "## Keypoints\n",
    "\n",
    "- RNNs and LSTMs allow neural networks to process data step-by-step\n",
    "- LSTMs retain long-term context using gated memory\n",
    "- Sequence models are widely used in time series, language, and biology:::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882af60d-5426-42b3-b852-7ff1d1e05654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
