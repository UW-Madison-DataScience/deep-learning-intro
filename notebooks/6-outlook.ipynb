{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1da7a5e",
   "metadata": {},
   "source": [
    "# Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8920cc",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- How does what I learned in this course translate to real-world problems?\n",
    "- How do I organise a deep learning project?\n",
    "- What are next steps to take after this course?\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand that what we learned in this course can be applied to real-world problems\n",
    "- Use best practices for organising a deep learning project\n",
    "- Identify next steps to take after this course\n",
    "\n",
    "You have come to the end of this course.\n",
    "In this episode we will look back at what we have learned so far, how to apply that to real-world problems, and identify\n",
    "next steps to take to start applying deep learning in your own projects.\n",
    "\n",
    "## Real-world application\n",
    "To introduce the core concepts of deep learning we have used quite simple machine learning problems.\n",
    "But how does what we learned so far apply to real-world applications?\n",
    "\n",
    "To illustrate that what we learned is actually the basis of successful applications in research,\n",
    "we will have a look at an example from the field of [cheminformatics](https://en.wikipedia.org/wiki/Cheminformatics).\n",
    "\n",
    "\n",
    "We will have a look at [this notebook](https://github.com/matchms/ms2deepscore/blob/0.4.0/notebooks/MS2DeepScore_tutorial.ipynb).\n",
    "It is part of the codebase for [this paper](https://doi.org/10.1186/s13321-021-00558-4).\n",
    "\n",
    "In short, the deep learning problem is that of finding out how similar two molecules are in terms of their molecular properties,\n",
    "based on their mass spectrum.\n",
    "You can compare this to comparing two pictures of animals, and predicting how similar they are.\n",
    "\n",
    "A Siamese neural network is used to solve the problem.\n",
    "In a Siamese neural network you have two input vectors, let's say two images of animals or two mass spectra.\n",
    "They pass through a base network. Instead of outputting a class or number with one or a few output neurons, the output layer\n",
    "of the base network is a whole vector of for example 100 neurons. After passing through the base network, you end up with two of these\n",
    "vectors representing the two inputs. The goal of the base network is to output a meaningful representation of the input (this is called an embedding).\n",
    "The next step is to compute the cosine similarity between these two output vectors,\n",
    "cosine similarity is a measure for how similar two vectors are to each other, ranging from 0 (completely different) to 1 (identical).\n",
    "This cosine similarity is compared to the actual similarity between the two inputs and this error is used to update the weights in the network.\n",
    "\n",
    "Don't worry if you do not fully understand the deep learning problem and the approach that is taken here.\n",
    "We just want you to appreciate that you already learned enough to be able to do this yourself in your own domain.\n",
    "\n",
    "## Challenge: Exercise: A real-world deep learning application\n",
    "\n",
    "1. Looking at the 'Model training' section of the notebook, what do you recognize from what you learned in this course?\n",
    "2. Can you identify the different steps of the deep learning workflow in this notebook?\n",
    "3. (Optional): Try to understand the neural network architecture from the first figure of [the paper](https://doi.org/10.1186/s13321-021-00558-4).\n",
    "    a. Why are there 10.000 neurons in the input layer?\n",
    "    b. What do you think would happen if you would decrease the size of spectral embedding layer drastically, to for example 5 neurons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716dade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e01702a7",
   "metadata": {},
   "source": [
    "Hopefully you can appreciate that what you learned in this course, can be applied to real-world problems as well.\n",
    "\n",
    "## Callout: Extensive data preparation\n",
    "\n",
    "You might have noticed that the data preparation for this example is much more extensive than what we have done so far\n",
    "in this course. This is quite common for applied deep learning projects. It is said that 90% of the time in a\n",
    "deep learning problem is spent on data preparation, and only 10% on modeling!\n",
    "\n",
    "## Discussion: Bias and Evaluation\n",
    "\n",
    "Bias has been discussed in the context of machine learning, deep learning and artificial intelligence frequently and on various levels.\n",
    "That is because there are many aspects to bias.\n",
    "One the one hand, bias is very technical: a model can be biased towards certain classes or certain features.\n",
    "On the other hand, this can have very practical and severe impact on the users of a such a model;\n",
    "for instance when it comes to misclassification in relation to color of the skin or geographical location.\n",
    "\n",
    "If such biases are reflected in a dataset that is used for model validation and testing, you might not be able to see them.\n",
    "In order to get an evaluation that is representative for the diversity found in the real world, it is therefore important to use a test set that reflects this diversity as much as possible.\n",
    "\n",
    "The need for such a dataset as opposed to existing datasets that mostly presumed Western standards has been one of the motivations for creating the Dollar Street Dataset -- and why we have used it in this lesson.\n",
    "The creators [have shown](https://papers.nips.cc/paper_files/paper/2022/hash/5474d9d43c0519aa176276ff2c1ca528-Abstract-Datasets_and_Benchmarks.html) that more diversity in a training dataset can contribute to significant model improvements.\n",
    "A model trained on a more diverse dataset is more robust against unexpected occurrences.\n",
    "\n",
    "Therefore, it is important to fully understand the quantitative evaluation of a new model:\n",
    "it reflects the model's performance on the test set, but it does not say anything about how well that dataset represents the real world.\n",
    "Also be aware that such matters can be related to racism and other forms of discrimination.\n",
    "Depending on the use case, diversity can also refer to imbalance on other, more subtle and less sensitive dimensions.\n",
    "\n",
    "**Discuss the following statement with your neighbors:**\n",
    "\n",
    "- What forms of bias and data imbalance can you think of?\n",
    "- How would they affect the performance of a deep learning model?\n",
    "\n",
    "## Discussion: Large Language Models and prompt engineering\n",
    "\n",
    "Large Language Models (LLMs) are deep learning models that are able to perform general-purpose language generation.\n",
    "They are trained on large amounts of texts, such all pages of Wikipedia. \n",
    "In recent years the quality of LLMs language understanding and generation has increased tremendously, and since the launch of generative chatbot ChatGPT in 2022 the power of LLMs is now appreciated by the general public.\n",
    "\n",
    "It is becoming more and more feasible to unleash this power in scientific research. For example, the authors of [Zheng et al. (2023)](https://doi.org/10.1021/acscentsci.3c01087) guided ChatGPT in the automation of extracting chemical information from a large amount of research articles. The authors did not implement a deep learning model themselves, but instead they designed the right input for ChatGPT (called a 'prompt') that would produce optimal outputs. This is called prompt engineering. A highly simplified example of such a prompt would be: \"Given compounds X and Y and context Z, what are the chemical details of the reaction?\"\n",
    "\n",
    "Developments in LLM research are moving fast, at the end of 2023 the newest ChatGPT version [could take images and sound as input](https://openai.com/blog/chatgpt-can-now-see-hear-and-speak). \n",
    "In theory, this means that you can solve the Dollar Street image classification problem from the previous episode by prompt engineering, with prompts similar to \"Which out of these categories: [LIST OF CATEGORIES] is depicted in the image\".\n",
    "\n",
    "**Discuss the following statement with your neighbors:**\n",
    "\n",
    "_In a few years most machine learning problems in scientific research can be solved with prompt engineering._\n",
    "\n",
    "## Organising deep learning projects\n",
    "As you might have noticed already in this course, deep learning projects can quickly become messy.\n",
    "Here follow some best practices for keeping your projects organized:\n",
    "\n",
    "### 1. Organise experiments in notebooks\n",
    "Jupyter notebooks are a useful tool for doing deep learning experiments.\n",
    "You can very easily modify your code bit by bit, and interactively look at the results.\n",
    "In addition you can explain why you are doing things in markdown cells.\n",
    "- As a rule of thumb do one approach or experiment in one notebook.\n",
    "- Give consistent and meaningful names to notebooks, such as: `01-all-cities-simple-cnn.ipynb`\n",
    "- Add a rationale on top and a conclusion on the bottom of each notebook\n",
    "\n",
    "[_Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks_](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007) provides further advice on how to maximise the usefulness and reproducibility of experiments captured in a notebook.\n",
    "\n",
    "### 2. Use Python modules\n",
    "Code that is repeatedly used should live in a Python module and not be copied to multiple notebooks.\n",
    "You can import functions and classes from the module(s) in the notebooks.\n",
    "This way you can remove a lot of code definition from your notebooks and have a focus on the actual experiment.\n",
    "\n",
    "### 3. Keep track of your results in a central place\n",
    "Always evaluate your experiments in the same way, on the exact same test set.\n",
    "Document the results of your experiments in a consistent and meaningful way.\n",
    "You can use a simple spreadsheet such as this:\n",
    "\n",
    "| MODEL NAME              | MODEL DESCRIPTION                          | RMSE | TESTSET NAME  | GITHUB COMMIT | COMMENTS |\n",
    "|-------------------------|--------------------------------------------|------|---------------|---------------|----------|\n",
    "| weather_prediction_v1.0 | Basel features only, 10 years. nn: 100-50  | 3.21 | 10_years_v1.0 |  ed28d85      |          |\n",
    "| weather_prediction_v1.1 | all features, 10 years. nn: 100-50         | 3.35 | 10_years_v1.0 |  4427b78      |          |\n",
    "\n",
    "You could also use a tool such as [Weights and Biases](https://wandb.ai/site) for this.\n",
    "\n",
    "## Callout: Cookiecutter data science\n",
    "\n",
    "If you want to get more pointers for organising deep learning, or data science projects in general,\n",
    "we recommend [Cookiecutter data science](https://drivendata.github.io/cookiecutter-data-science/).\n",
    "It is a template for initiating an organized data science project folder structure\n",
    "that you can adapt to your own needs.\n",
    "## Next steps\n",
    "You now understand the basic principles of deep learning and are able to implement your own deep learning pipelines in Python.\n",
    "But there is still so much to learn and do!\n",
    "\n",
    "Here are some suggestions for next steps you can take in your endeavor to become a deep learning expert:\n",
    "\n",
    "* Learn more by going through a few of [the learning resources we have compiled for you](learners/reference.md#external-references)\n",
    "* Apply what you have learned to your own projects. Use the deep learning workflow to structure your work.\n",
    "Start as simple as possible, and incrementally increase the complexity of your approach.\n",
    "* Compete in a [Kaggle competition](https://www.kaggle.com/competitions) to practice what you have learned.\n",
    "* Get access to a GPU. Your deep learning experiments will progress much quicker if you have to wait for your network to train\n",
    "in a few seconds instead of hours (which is the order of magnitude of speedup you can expect from training on a GPU instead of CPU).\n",
    "Tensorflow/Keras will automatically detect and use a GPU if it is available on your system without any code changes.\n",
    "A simple and quick way to get access to a GPU is to use [Google Colab](https://colab.google/)\n",
    "\n",
    "## Keypoints\n",
    "\n",
    "- Although the data preparation and model architectures are somewhat more complex,\n",
    "what we have learned in this course can directly be applied to real-world problems\n",
    "- Use what you have learned in this course as a basis for your own learning trajectory in the world of deep learning"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
